# Relat√≥rio do Projeto: Classificador de Imagens com Redes Neurais Artificiais ü§ñüñºÔ∏è

**Aluno:** [C√≠cero Junior/cicerojr10]
**Data:** [Data da Conclus√£o: 31/05/2025 | Postagem: 01/06/2025]

## 1. Resumo do Projeto üéØ

Este projeto teve como objetivo desenvolver um classificador de imagens utilizando Redes Neurais Convolucionais (CNNs). O processo envolveu a prepara√ß√£o de um dataset customizado, a constru√ß√£o de um modelo de Deep Learning, o treinamento iterativo deste modelo e a aplica√ß√£o de t√©cnicas para otimizar seu desempenho e combater o overfitting. O ambiente de desenvolvimento principal foi o Google Colab, com dados hospedados no Google Drive.

## 2. Fases do Projeto üõ†Ô∏è

O desenvolvimento do projeto pode ser dividido nas seguintes fases principais:

### 2.1. Prepara√ß√£o do Dataset e Ambiente Inicial
* **Defini√ß√£o do Dataset:** O projeto iniciou com a necessidade de um dataset com pelo menos duas classes de objetos, cada uma contendo no m√≠nimo 100 imagens com resolu√ß√£o m√≠nima de 400x400 pixels.
* **Configura√ß√£o do Ambiente:** Foi escolhido o Google Colab como plataforma de desenvolvimento, aproveitando seus recursos de GPU e ambiente Python pr√©-configurado.
* **Gerenciamento de Dados:** As imagens foram armazenadas no Google Drive. Desenvolvemos um script em Python para automatizar o download e a descompacta√ß√£o do dataset (arquivos `.zip`) diretamente no ambiente do Colab. Esta etapa envolveu a resolu√ß√£o de desafios como o manuseio de arquivos grandes e a intera√ß√£o com o Google Drive via c√≥digo (utilizando a biblioteca `gdown`).

### 2.2. Carregamento e Pr√©-processamento dos Dados
* **Carregamento:** Utilizamos a fun√ß√£o `tf.keras.utils.image_dataset_from_directory` do TensorFlow/Keras para carregar as imagens de forma eficiente, inferindo os r√≥tulos a partir da estrutura de diret√≥rios e dividindo os dados em conjuntos de treino e valida√ß√£o.
* **Redimensionamento:** As imagens foram redimensionadas para um tamanho padr√£o (ex: 150x150 pixels) para entrada na rede neural.
* **Normaliza√ß√£o:** Os valores dos pixels foram reescalados do intervalo [0, 255] para [0, 1] usando uma camada `layers.Rescaling` para facilitar o treinamento.
* **Data Augmentation:** Para aumentar a robustez do modelo e a variabilidade dos dados de treino, foi implementada uma camada de `data_augmentation` com transforma√ß√µes como `RandomFlip`, `RandomRotation`, `RandomZoom`, `RandomBrightness` e `RandomContrast`. A intensidade dessas transforma√ß√µes foi ajustada iterativamente.

### 2.3. Constru√ß√£o do Modelo de Rede Neural Convolucional (CNN)
* Foi constru√≠da uma CNN do zero utilizando a API Sequencial do Keras.
* A arquitetura base consistiu em:
    * Camada de Data Augmentation.
    * Camada de Rescaling.
    * M√∫ltiplos blocos convolucionais (geralmente `Conv2D` + `MaxPooling2D`). Experimentamos com diferentes n√∫meros de filtros (32, 64, 128).
    * Camadas de `Dropout` adicionadas estrategicamente para regulariza√ß√£o.
    * Camada `Flatten`.
    * Camadas `Dense` (totalmente conectadas). Experimento com diferentes n√∫meros de neur√¥nios (ex: 128, 256).
    * Camada `Dense` de sa√≠da com ativa√ß√£o `softmax`.

### 2.4. Compila√ß√£o, Treinamento e Otimiza√ß√£o Iterativa
* **Compila√ß√£o:** O modelo foi compilado usando o otimizador `adam` (com ajuste na taxa de aprendizado), a fun√ß√£o de perda `categorical_crossentropy` e a m√©trica `accuracy`.
* **Treinamento:** O modelo foi treinado com o m√©todo `model.fit()`.
* **Callbacks:**
    * `EarlyStopping`: Implementado para monitorar `val_loss` e, posteriormente, `val_accuracy`, interrompendo o treinamento otimamente e restaurando os melhores pesos.
    * `ReduceLROnPlateau`: Adicionado para reduzir dinamicamente a taxa de aprendizado quando a `val_loss` estagnava.
* **An√°lise e Itera√ß√£o:** Ap√≥s cada rodada, gr√°ficos de acur√°cia e perda foram analisados para guiar os ajustes no modelo e nos hiperpar√¢metros.

## 3. Tecnologias e Bibliotecas Utilizadas üíª

* **Linguagem de Programa√ß√£o:** Python
* **Ambiente Principal:** Google Colaboratory (Colab)
* **Framework de Deep Learning:** TensorFlow com a API Keras
* **Manipula√ß√£o de Dados e Arquivos:**
    * `gdown`
    * `zipfile`
    * `os`, `shutil`, `pathlib`
* **Visualiza√ß√£o:** Matplotlib
* **Computa√ß√£o Num√©rica:** NumPy (usado implicitamente pelo TensorFlow)

## 4. √Åreas de Conhecimento Aplicadas üß†

Este projeto abrangeu diversas √°reas fundamentais, incluindo:

* **Intelig√™ncia Artificial (IA):** O campo mais amplo.
* **Machine Learning (ML):** Especificamente, aprendizado supervisionado para classifica√ß√£o de imagens.
* **Deep Learning:** Utiliza√ß√£o de Redes Neurais Artificiais Profundas (CNNs).
* **Ci√™ncia de Dados (Data Science):**
    * Prepara√ß√£o e gerenciamento de datasets.
    * Pr√©-processamento e aumento de dados (Data Augmentation).
    * Modelagem preditiva e avalia√ß√£o de modelos.
    * Processo iterativo e experimental.
* **Vis√£o Computacional (Computer Vision):** Classifica√ß√£o de imagens como tarefa central.
* **Redes Neurais Artificiais (ANNs):** Foco em Redes Neurais Convolucionais (CNNs).
* **Programa√ß√£o em Python:** Implementa√ß√£o de todas as etapas.
* **Boas Pr√°ticas de Desenvolvimento (Iniciais):** Uso de ambiente interativo, organiza√ß√£o, tratamento de erros e inten√ß√£o de versionamento.
* **Cloud Computing:** Uso do Google Colab e Google Drive.

## 5. Resultados e Conclus√µes (do Ponto Atual) üìà

Ao longo de v√°rias itera√ß√µes de treinamento e ajuste de hiperpar√¢metros (arquitetura da rede, taxas de dropout, estrat√©gias de data augmentation, taxa de aprendizado, callbacks), o modelo alcan√ßou uma **acur√°cia de valida√ß√£o m√°xima de 85%**.

O processo demonstrou a import√¢ncia:
* Da qualidade e prepara√ß√£o dos dados.
* De t√©cnicas de regulariza√ß√£o (Dropout, Data Augmentation) para combater o overfitting.
* Do uso de callbacks como `EarlyStopping` e `ReduceLROnPlateau` para otimizar o processo de treinamento.
* Da an√°lise iterativa dos resultados para guiar as pr√≥ximas etapas de otimiza√ß√£o.

Embora o objetivo inicial mais ambicioso de 90% de acur√°cia n√£o tenha sido atingido com a arquitetura e estrat√©gias atuais (sem Transfer Learning), o projeto serviu como uma excelente plataforma para aplicar e entender os conceitos fundamentais de constru√ß√£o e treinamento de Redes Neurais Convolucionais. Pr√≥ximos passos para buscar maior acur√°cia poderiam incluir o uso de Transfer Learning, aumento do dataset ou um ajuste ainda mais fino e sistem√°tico dos hiperpar√¢metros.

---
